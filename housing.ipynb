{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "housing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gyjuU-CkX5TE"
      },
      "source": [
        "# HOUSING KAGGLE REGRESSION PROBLEM\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iiXZLrKKo9N5"
      },
      "source": [
        "### First regression model\n",
        "OOTB Random Forest Regressor to be used as our ruler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CyX29qpfEn9_",
        "colab": {}
      },
      "source": [
        "# loading dataset into the script\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/LeoFernanndes/datasets/master/housing_train.csv')\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#warnings.filterwarnings(action='once')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HtDM84ohEn-G",
        "colab": {}
      },
      "source": [
        "# a generic method to arrange the variables\n",
        "housing = df.copy()\n",
        "\n",
        "# starting by filling the blank spaces \n",
        "for column in housing.columns:\n",
        "    if housing[column].dtype != 'object':\n",
        "        housing[column].fillna(housing[column].median(), inplace= True)\n",
        "    else:\n",
        "        housing[column] = housing[column].astype('str')\n",
        "        housing[column].fillna(housing[column].mode(), inplace= True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zdY4xgRqEn-N",
        "colab": {}
      },
      "source": [
        "## fitting the first regression model \n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "# splitting the dependent (predictor) and dependent (target) features\n",
        "x = housing.drop(['SalePrice'], axis= 1)\n",
        "y = housing['SalePrice']\n",
        "\n",
        "# applying label encoding\n",
        "le = LabelEncoder()\n",
        "for column in x.columns:\n",
        "    if x[column].dtype == 'object':\n",
        "        x[column] = le.fit_transform(x[column])\n",
        "\n",
        "# splitting the data into train and test\n",
        "x_treino, x_teste, y_treino, y_teste = train_test_split(x, y, test_size= 0.2, random_state= 55)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Az23RrjmEn-T",
        "outputId": "151f1ca0-ced7-4926-acc8-059f5318924b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# calling the regressor\n",
        "reg1 = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
        "           max_features='auto', max_leaf_nodes=None,\n",
        "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "           min_samples_leaf=1, min_samples_split=2,\n",
        "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
        "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
        "\n",
        "# fitting the model\n",
        "reg1.fit(x_treino, y_treino)\n",
        "\n",
        "# making the prediction of target in trein set\n",
        "prev = reg1.predict(x_teste)\n",
        "\n",
        "\n",
        "# error evaluation\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "erro = np.sqrt(mean_squared_log_error(y_teste, prev))\n",
        "\n",
        "## RMSLE obtained from our first model\n",
        "erro"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13061496412307086"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ktuyZBJwEn-W",
        "colab": {}
      },
      "source": [
        "# making the predictions over submission set\n",
        "# loading submission data\n",
        "url_data = 'https://raw.githubusercontent.com/LeoFernanndes/datasets/master/housing_test.csv'\n",
        "data = pd.read_csv(url_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n7SGogTzEn-a",
        "colab": {}
      },
      "source": [
        "# filling in the gaps in the same manner as used in the trein set\n",
        "for column in data.columns:\n",
        "    if data[column].dtype != 'object':\n",
        "        data[column].fillna(data[column].mean(), inplace= True)\n",
        "    else:\n",
        "        data[column] = data[column].astype('str')\n",
        "        data[column].fillna(data[column].mode(), inplace= True)\n",
        "\n",
        "# using label encoder over the test set the same way as used in trein set \n",
        "le = LabelEncoder()\n",
        "for column in data.columns:\n",
        "    if data[column].dtype == 'object':\n",
        "        data[column] = le.fit_transform(data[column])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zJGc0002En-d",
        "colab": {}
      },
      "source": [
        "# df for kaggle evaluation\n",
        "final = reg1.predict(data)\n",
        "envio = pd.DataFrame({'Id': data['Id'], 'SalePrice': final})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uPF-40m4piQJ"
      },
      "source": [
        "Of corse we didn't expect here anything extremely accurate for some reasons:\n",
        "* The model itself was implemented just out of the box.\n",
        "* The approach to fill the missing values using mode to categorical features and mean in the case of numerical ones didn't take in account any exploratory analysis of the variables.\n",
        "* No methodology for feature engineering and in order to tune model precision by finding cerrelations or opportunities to dimensional reduction.\n",
        "* All categorical features were encoded with the same method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "amWBKB4LjpV2"
      },
      "source": [
        "## Model improvement attempts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYVmJsopWIWF",
        "colab_type": "text"
      },
      "source": [
        "Using feature importances to find the most relevant ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "crh46d0MXi5K",
        "colab": {}
      },
      "source": [
        "featimp = pd.Series(reg1.feature_importances_, index= x_teste.columns).sort_values(ascending=False)\n",
        "featimp.index\n",
        "feat_upper = featimp.index[0: 24]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZJTVDj33Xi5P",
        "colab": {}
      },
      "source": [
        "# novo x\n",
        "x2 = x.copy()\n",
        "y2 = y.copy()\n",
        "\n",
        "# novo encoding\n",
        "le2 = LabelEncoder()\n",
        "for column in x2.columns:\n",
        "    if x2[column].dtype == 'object':\n",
        "        x2[column] = le2.fit_transform(x2[column])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsRW-mpuWeTi",
        "colab_type": "text"
      },
      "source": [
        "We're now going to use XGBoost Regressor just out of the box like Random Forest above.\n",
        "\n",
        "But first let's perform an algorithm responsible for selecting the optmal number of features to be used based on that naive feature importances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7_HTVzFQy2Fc",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "y_axis = []\n",
        "x_axis = []\n",
        "\n",
        "it = 80\n",
        "\n",
        "for m in(range(it)):\n",
        "\n",
        "  error2_ = 0\n",
        "  n = 10\n",
        "  \n",
        "  for i in(range(n)):\n",
        "    # chamando o modelo\n",
        "    reg2 = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
        "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
        "             n_jobs=1, nthread=None, objective='reg:squarederror', random_state=0,\n",
        "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "             silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "    x2_treino, x2_teste, y2_treino, y2_teste = train_test_split(x2, y2, test_size= 0.30, random_state= i**2)\n",
        "\n",
        "    # ajustando um modelo\n",
        "    reg2.fit(x2_treino[featimp.index[0: m+1]], y2_treino)\n",
        "\n",
        "    # fazendo a previsao\n",
        "    prev2 = reg2.predict(x2_teste[featimp.index[0: m+1]])\n",
        "\n",
        "\n",
        "    # avaliando o erro\n",
        "    from sklearn.metrics import mean_squared_log_error\n",
        "    import numpy as np\n",
        "\n",
        "\n",
        "    error2 = np.sqrt(mean_squared_log_error(y2_teste, prev2))\n",
        "    error2_ += error2\n",
        "\n",
        "  y_axis.append(error2_/n)\n",
        "  x_axis.append(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXJM9PEuXrYy",
        "colab_type": "text"
      },
      "source": [
        "Let's see how the error vary according to the number os features used in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_hVLg14A09jq",
        "outputId": "e6e94761-bdc8-4d77-c553-d88b57ce47be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(x_axis, y_axis)\n",
        "plt.title('RMSLE')\n",
        "plt.xlabel('Number of sorted features')\n",
        "plt.ylabel('Error')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhfZXt58AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX973LvgYM8_",
        "colab_type": "text"
      },
      "source": [
        "List of the best 5 numbers of sorted variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xuFoEpSY2vYk",
        "outputId": "c3797055-2642-4445-fb36-d84e4ef4cb48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "error_df = pd.DataFrame({'n of features': x_axis,\n",
        "                         'error': y_axis}).sort_values(by= ['error'], ascending= True)\n",
        "\n",
        "error_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n of features</th>\n",
              "      <th>error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>79</td>\n",
              "      <td>0.134536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>77</td>\n",
              "      <td>0.134536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>76</td>\n",
              "      <td>0.134536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>78</td>\n",
              "      <td>0.134536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>65</td>\n",
              "      <td>0.134563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>75</td>\n",
              "      <td>0.134605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>35</td>\n",
              "      <td>0.134620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>62</td>\n",
              "      <td>0.134647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>63</td>\n",
              "      <td>0.134720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>66</td>\n",
              "      <td>0.134733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>64</td>\n",
              "      <td>0.134795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>71</td>\n",
              "      <td>0.134797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>70</td>\n",
              "      <td>0.134822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>72</td>\n",
              "      <td>0.134856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>37</td>\n",
              "      <td>0.134857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>73</td>\n",
              "      <td>0.134863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>74</td>\n",
              "      <td>0.134863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>36</td>\n",
              "      <td>0.134908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>58</td>\n",
              "      <td>0.134963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>67</td>\n",
              "      <td>0.134998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>68</td>\n",
              "      <td>0.134998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>69</td>\n",
              "      <td>0.134998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>57</td>\n",
              "      <td>0.135045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>56</td>\n",
              "      <td>0.135131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>55</td>\n",
              "      <td>0.135131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>44</td>\n",
              "      <td>0.135134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>40</td>\n",
              "      <td>0.135156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>39</td>\n",
              "      <td>0.135165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>45</td>\n",
              "      <td>0.135176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>42</td>\n",
              "      <td>0.135187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>41</td>\n",
              "      <td>0.135211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>38</td>\n",
              "      <td>0.135229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>61</td>\n",
              "      <td>0.135254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>0.135293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>52</td>\n",
              "      <td>0.135354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>53</td>\n",
              "      <td>0.135381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>51</td>\n",
              "      <td>0.135403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>59</td>\n",
              "      <td>0.135425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>48</td>\n",
              "      <td>0.135494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>54</td>\n",
              "      <td>0.135591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>60</td>\n",
              "      <td>0.135641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>47</td>\n",
              "      <td>0.135680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>49</td>\n",
              "      <td>0.135720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>34</td>\n",
              "      <td>0.135731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>46</td>\n",
              "      <td>0.135825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>50</td>\n",
              "      <td>0.135907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>31</td>\n",
              "      <td>0.137320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>30</td>\n",
              "      <td>0.137468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>33</td>\n",
              "      <td>0.137622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>0.137657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>0.138433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>0.138567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>0.138663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>0.138707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>0.138877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>0.138970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>0.139030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>0.139102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>0.143832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>0.145055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>0.145228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>0.145374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>0.145645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>0.145713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>0.145798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>0.145872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>0.146086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>0.146305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>0.150259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>0.154206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>0.154364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0.155610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>0.163981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0.166569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0.168247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.173993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.177073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.186908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.201534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.229032</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    n of features     error\n",
              "79             79  0.134536\n",
              "77             77  0.134536\n",
              "76             76  0.134536\n",
              "78             78  0.134536\n",
              "65             65  0.134563\n",
              "75             75  0.134605\n",
              "35             35  0.134620\n",
              "62             62  0.134647\n",
              "63             63  0.134720\n",
              "66             66  0.134733\n",
              "64             64  0.134795\n",
              "71             71  0.134797\n",
              "70             70  0.134822\n",
              "72             72  0.134856\n",
              "37             37  0.134857\n",
              "73             73  0.134863\n",
              "74             74  0.134863\n",
              "36             36  0.134908\n",
              "58             58  0.134963\n",
              "67             67  0.134998\n",
              "68             68  0.134998\n",
              "69             69  0.134998\n",
              "57             57  0.135045\n",
              "56             56  0.135131\n",
              "55             55  0.135131\n",
              "44             44  0.135134\n",
              "40             40  0.135156\n",
              "39             39  0.135165\n",
              "45             45  0.135176\n",
              "42             42  0.135187\n",
              "41             41  0.135211\n",
              "38             38  0.135229\n",
              "61             61  0.135254\n",
              "43             43  0.135293\n",
              "52             52  0.135354\n",
              "53             53  0.135381\n",
              "51             51  0.135403\n",
              "59             59  0.135425\n",
              "48             48  0.135494\n",
              "54             54  0.135591\n",
              "60             60  0.135641\n",
              "47             47  0.135680\n",
              "49             49  0.135720\n",
              "34             34  0.135731\n",
              "46             46  0.135825\n",
              "50             50  0.135907\n",
              "31             31  0.137320\n",
              "30             30  0.137468\n",
              "33             33  0.137622\n",
              "32             32  0.137657\n",
              "27             27  0.138433\n",
              "26             26  0.138567\n",
              "29             29  0.138663\n",
              "28             28  0.138707\n",
              "22             22  0.138877\n",
              "23             23  0.138970\n",
              "25             25  0.139030\n",
              "24             24  0.139102\n",
              "21             21  0.143832\n",
              "19             19  0.145055\n",
              "14             14  0.145228\n",
              "13             13  0.145374\n",
              "20             20  0.145645\n",
              "17             17  0.145713\n",
              "18             18  0.145798\n",
              "16             16  0.145872\n",
              "15             15  0.146086\n",
              "12             12  0.146305\n",
              "11             11  0.150259\n",
              "10             10  0.154206\n",
              "9               9  0.154364\n",
              "8               8  0.155610\n",
              "7               7  0.163981\n",
              "6               6  0.166569\n",
              "5               5  0.168247\n",
              "4               4  0.173993\n",
              "3               3  0.177073\n",
              "2               2  0.186908\n",
              "1               1  0.201534\n",
              "0               0  0.229032"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edwRKXTyYsHW",
        "colab_type": "text"
      },
      "source": [
        "Choosen set of features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zanuI3drUfUA",
        "colab_type": "code",
        "outputId": "5e43ecdc-a74c-481f-9d13-533f4b579ecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# list\n",
        "limit = list(error_df['n of features'])[0]\n",
        "featimp.index[0: limit]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['OverallQual', 'GrLivArea', 'TotalBsmtSF', 'BsmtFinSF1', '2ndFlrSF',\n",
              "       'GarageArea', '1stFlrSF', 'LotArea', 'YearBuilt', 'GarageCars',\n",
              "       'TotRmsAbvGrd', 'Neighborhood', 'YearRemodAdd', 'FullBath',\n",
              "       'KitchenQual', 'LotFrontage', 'BsmtQual', 'OpenPorchSF', 'BsmtUnfSF',\n",
              "       'GarageYrBlt', 'MasVnrArea', 'CentralAir', 'OverallCond', 'Id',\n",
              "       'GarageType', 'WoodDeckSF', 'GarageFinish', 'ExterQual', 'MoSold',\n",
              "       'LotShape', 'Fireplaces', 'BsmtFinType1', 'BsmtExposure', 'FireplaceQu',\n",
              "       'MSZoning', 'BedroomAbvGr', 'SaleCondition', 'YrSold', 'Exterior1st',\n",
              "       'MSSubClass', 'Exterior2nd', 'BsmtHalfBath', 'LandContour', 'HalfBath',\n",
              "       'LotConfig', 'MasVnrType', 'BsmtFullBath', 'PoolArea', 'SaleType',\n",
              "       'HouseStyle', 'RoofStyle', 'LandSlope', 'HeatingQC', 'EnclosedPorch',\n",
              "       'ExterCond', 'ScreenPorch', 'PoolQC', 'KitchenAbvGr', 'BsmtFinType2',\n",
              "       'Functional', 'BsmtFinSF2', 'Alley', 'Condition1', 'Foundation',\n",
              "       'Fence', 'BldgType', 'PavedDrive', 'RoofMatl', 'GarageQual',\n",
              "       'GarageCond', '3SsnPorch', 'BsmtCond', 'Electrical', 'LowQualFinSF',\n",
              "       'MiscVal', 'Heating', 'Condition2', 'MiscFeature', 'Street'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ohmLEF783z8",
        "colab": {}
      },
      "source": [
        "# loading train data\n",
        "dataframe = pd.read_csv('https://raw.githubusercontent.com/LeoFernanndes/datasets/master/housing_train.csv')\n",
        "feat_test = featimp.index[0: limit]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rnu2LvGm8_zn",
        "colab": {}
      },
      "source": [
        "df = dataframe[feat_test]\n",
        "\n",
        "le2 = LabelEncoder()\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object':\n",
        "      df[column] = df[column].astype('str')\n",
        "      df[column] = le2.fit_transform(df[column])\n",
        "\n",
        "x = df\n",
        "y = dataframe.iloc[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-J34NML1_JUl",
        "colab": {}
      },
      "source": [
        "# loading submission data\n",
        "url_data = 'https://raw.githubusercontent.com/LeoFernanndes/datasets/master/housing_test.csv'\n",
        "data1 = pd.read_csv(url_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D4F0H1Yb_qR1",
        "colab": {}
      },
      "source": [
        "data = data1[feat_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P67aWAqQ_UhH",
        "colab": {}
      },
      "source": [
        "# filling in the gaps\n",
        "for column in data.columns:\n",
        "    if data[column].dtype != 'object':\n",
        "        data[column].fillna(data[column].median(), inplace= True)\n",
        "    else:\n",
        "        data[column] = data[column].astype('str')\n",
        "        data[column].fillna(data[column].mode(), inplace= True)\n",
        "\n",
        "# label encoding test data the same way as train\n",
        "for column in data.columns:\n",
        "    if data[column].dtype == 'object':\n",
        "        data[column] = le2.fit_transform(data[column])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nhaPNjJ16X6Q",
        "colab": {}
      },
      "source": [
        "# calling the regressor\n",
        "\n",
        "reg = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
        "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
        "             n_jobs=1, nthread=None, objective='reg:squarederror', random_state=0,\n",
        "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "             silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "\n",
        "# fittin the model\n",
        "reg.fit(x, y)\n",
        "\n",
        "\n",
        "# making the prediction\n",
        "prev = reg.predict(data)\n",
        "\n",
        "# creating submission dataset\n",
        "envio = pd.DataFrame({'Id': data1['Id'], 'SalePrice': prev})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6b-gHVOrE4_6",
        "colab": {}
      },
      "source": [
        "# saving the df in a csv to be uploaded\n",
        "\n",
        "#path = r'C:\\Users\\Avell\\Desktop\\Python\\github\\datasets'\n",
        "#nome = '\\housing_submission_2.csv'\n",
        "#envio.to_csv(path+nome, index= False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h55cdn0B8uVK",
        "colab_type": "text"
      },
      "source": [
        "##References\n",
        "\n",
        "Complete Guide to Parameter Tuning in XGBoost with codes in Python\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
        "\n",
        "Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
        "\n",
        "Understanding the Bias-Variance Tradeoff\n",
        "\n",
        "https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\n",
        "\n",
        "Fundamental Techniques of Feature Engineering for Machine Learning\n",
        "\n",
        "https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114"
      ]
    }
  ]
}